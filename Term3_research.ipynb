{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WKZ3WaL-fQ_p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/opt/homebrew/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n",
            "  Referenced from: <D9493EF5-8DAB-3A5D-85D5-684F04544B84> /opt/homebrew/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Expected in:     <BB02660F-1D5B-3388-B48B-486877D726F6> /opt/homebrew/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "from numpy import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKBKcbIlfwxp",
        "outputId": "ad0fb7dd-6b92-4f27-b079-3d574c0f5413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using mps device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "bSLPErtAlVGC"
      },
      "outputs": [],
      "source": [
        "N = 3\n",
        "k1 = 2**N\n",
        "L_enc = 3\n",
        "L_dec = 3\n",
        "I_dec = 3\n",
        "N_enc = 96\n",
        "N_dec = 96\n",
        "n1 = 16\n",
        "\n",
        "encoder_learning_rate = 2e-5\n",
        "decoder_learning_rate = 2e-5\n",
        "noise = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "g-osyrJSgC12"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        layers.append(nn.Linear(k1, N_enc))\n",
        "        layers.append(nn.SELU())\n",
        "        for i in range(L_enc):\n",
        "            layers.append(nn.Linear(N_enc, N_enc))\n",
        "            layers.append(nn.SELU())\n",
        "        layers.append(nn.Linear(N_enc, n1))\n",
        "\n",
        "        #layers.append(nn.Tanh())\n",
        "\n",
        "        self.fcnn = nn.Sequential(\n",
        "            *layers\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.fcnn(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # it's a set of layers of one decoder out of I_dec decoders\n",
        "        layers = []\n",
        "\n",
        "        # first decoder \n",
        "        layers.append(nn.Linear(n1, N_dec))\n",
        "        layers.append(nn.SELU())\n",
        "        for i in range(L_enc):\n",
        "            layers.append(nn.Linear(N_dec, N_dec))\n",
        "            layers.append(nn.SELU())\n",
        "        \n",
        "        # second & third decoders\n",
        "        for j in range(2):\n",
        "            for i in range(L_enc + 1):\n",
        "                layers.append(nn.Linear(N_dec, N_dec))\n",
        "                layers.append(nn.SELU())\n",
        "\n",
        "        # last decoder\n",
        "        for i in range(L_enc):\n",
        "            layers.append(nn.Linear(N_dec, N_dec))\n",
        "            layers.append(nn.SELU())\n",
        "        layers.append(nn.Linear(N_dec, k1))\n",
        "        \n",
        "        #layers.append(nn.Tanh())\n",
        "\n",
        "        self.fcnn = nn.Sequential(\n",
        "            *layers\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.fcnn(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PowerNormaliser:\n",
        "    def normalize(self, tensor):\n",
        "        #print(tensor.shape)\n",
        "        norm = torch.norm(tensor, p=2)\n",
        "        normalized_vector = tensor / norm\n",
        "        return normalized_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.2673, 0.5345, 0.8018])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "p = PowerNormaliser()\n",
        "print(p.normalize(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "gwzbjvvzx4yr"
      },
      "outputs": [],
      "source": [
        "class Channel():\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def add_noise(self, data):\n",
        "        noise = random.normal(0, self.sigma, data.shape)\n",
        "        return data + torch.from_numpy(noise).to(dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Binarise():\n",
        "    def __init__(self):\n",
        "        self.threshold = 0\n",
        "    def binarise(self, data):\n",
        "        return (data > self.threshold).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "kuEsoz7Gm3OZ"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder()\n",
        "awgn_channel = Channel(noise)\n",
        "decoder = Decoder()\n",
        "normaliser = PowerNormaliser()\n",
        "binarise = Binarise()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "NNLznqvUnOK3"
      },
      "outputs": [],
      "source": [
        "enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = encoder_learning_rate)\n",
        "#enc_scheduler = torch.optim.lr_scheduler.ExponentialLR(enc_optimizer, gamma=0.95)\n",
        "dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = decoder_learning_rate)\n",
        "#dec_scheduler = torch.optim.lr_scheduler.ExponentialLR(dec_optimizer, gamma=0.95)\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples = 2**k1\n",
        "training_epochs = 10\n",
        "num_epochs = 1000\n",
        "batch_size = 1\n",
        "\n",
        "# x_train = torch.rand((num_samples, k1))\n",
        "# y_train = x_train\n",
        "# dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "# train_loader = DataLoader(dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#torch.randint(low = -1, high = )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder training, loss: 0.5788, epoch: 0\n",
            "Encoder training, loss: 0.3138, epoch: 0\n",
            "\n",
            "Decoder training, loss: 0.4275, epoch: 1\n",
            "Encoder training, loss: 0.4383, epoch: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# implement the training loop for the encoder part\n",
        "# at the same time the decoder part is frozen\n",
        "# the code should use train_loader to get the data\n",
        "# and the loss function should be loss_fn\n",
        "fl = True\n",
        "for i in range(training_epochs):\n",
        "#for i in range(1):\n",
        "    # generate random data for training\n",
        "    x_train = torch.randint(low=0, high=2, size=(num_samples, k1)).to(dtype=torch.float32)\n",
        "    y_train = x_train\n",
        "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    # implement the training loop for the decoder part\n",
        "    # at the same time the encoder part is frozen\n",
        "    # the code should use train_loader to get the data\n",
        "    # and the loss function should be loss_fn\n",
        "    \n",
        "    for epoch in range(200):\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            dec_optimizer.zero_grad()\n",
        "            \n",
        "            encoded = encoder(x_batch)\n",
        "            enc_norm = normaliser.normalize(encoded)\n",
        "            enc_norm_noise = awgn_channel.add_noise(enc_norm)\n",
        "            \n",
        "            decoded = decoder(enc_norm_noise)\n",
        "            # decoded_bin = binarise.binarise(decoded) - Can't do binarise here, because it's a part of the loss function\n",
        "            \n",
        "            loss = loss_fn(decoded, y_batch)\n",
        "            loss.backward()\n",
        "            dec_optimizer.step()\n",
        "            \n",
        "            #print(f\"{epoch=}, loss={loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Decoder training, loss: {loss.item():.4f}, epoch: {i}\")\n",
        "    \n",
        "    \n",
        "    x_train = torch.randint(low=0, high=2, size=(num_samples, k1)).to(dtype=torch.float32)\n",
        "    y_train = x_train\n",
        "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    \n",
        "    for epoch in range(100):\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            enc_optimizer.zero_grad()\n",
        "            \n",
        "            encoded = encoder(x_batch)\n",
        "            enc_norm = normaliser.normalize(encoded)\n",
        "            enc_norm_noisy = awgn_channel.add_noise(enc_norm)\n",
        "            \n",
        "            decoded = decoder(enc_norm_noisy)\n",
        "            \n",
        "            loss = loss_fn(decoded, y_batch)\n",
        "            loss.backward()\n",
        "            enc_optimizer.step()\n",
        "        \n",
        "    print(f\"Encoder training, loss: {loss.item():.4f}, epoch: {i}\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
